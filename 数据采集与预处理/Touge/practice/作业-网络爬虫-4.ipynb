{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ***************************************** Begin ************************************************\n",
    "# 请求网页\n",
    "def request_page(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status() \n",
    "        response.encoding = 'gb18030'  \n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"请求失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 解析网页\n",
    "def parse_page(html):\n",
    "    print(\"数据采集成功。\")\n",
    "    if html is None:\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    items = soup.find_all('tr')\n",
    "    \n",
    "    for item in items[1:]:  # 跳过表头\n",
    "        rank = item.find('td', class_='first').text.strip()\n",
    "        title = item.find('a', class_='list-title').text.strip()\n",
    "        hot = item.find('td', class_='last').text.strip()\n",
    "\n",
    "        print(\"排名：{0:^4}\\t标题：{1:^15}\\t热度：{2:^8}\".format(rank, title, hot))\n",
    "\n",
    "# ***************************************** End ************************************************\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = 'http://top.baidu.com/buzz?b=1&fr=20811'\n",
    "    # 需要修改为自己的 Header 信息\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0'}\n",
    "    html = request_page(url, headers)\n",
    "    parse_page(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 函数1：请求网页\n",
    "def page_request(session, url, ua):\n",
    "    response = session.get(url, headers=ua)\n",
    "    html = response.content.decode('utf-8')\n",
    "    return html\n",
    "\n",
    "# 函数2：解析网页\n",
    "def page_parse(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    info = soup.select('body > div.main3 > div.left > div.sons > div.cont')\n",
    "    sentence = soup.select('div.left > div.sons > div.cont > a:nth-of-type(1)')\n",
    "\n",
    "    sentence_list = []\n",
    "    href_list = []\n",
    "    \n",
    "    for i in range(len(info)):\n",
    "        curInfo = ''.join(info[i].get_text().split('\\n'))\n",
    "        sentence_list.append(curInfo)\n",
    "        href = sentence[i].get('href')\n",
    "        href_list.append(\"https://so.gushiwen.org\" + href)\n",
    "\n",
    "    return [href_list, sentence_list]\n",
    "\n",
    "# 保存诗句到文件\n",
    "def save_txt(filename, info_list):\n",
    "    with open(filename, 'a', encoding='utf-8') as txt_file:\n",
    "        for element in info_list:\n",
    "            txt_file.write(json.dumps(element, ensure_ascii=False) + '\\n\\n')\n",
    "\n",
    "# 子网页处理函数：请求并解析子网页\n",
    "def sub_page_request_parse(session, urls, ua):\n",
    "    sub_html = [page_request(session, url, ua) for url in urls]\n",
    "    poem_list = []\n",
    "    \n",
    "    for html in sub_html:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        poem = soup.select('div.left > div.sons > div.cont > div.contson')\n",
    "        if poem:\n",
    "            poem_list.append(poem[0].get_text().strip())\n",
    "\n",
    "    return poem_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"****************开始爬取古诗文网站******************\")\n",
    "    ua = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'}\n",
    "    \n",
    "    session = requests.Session()  # Use session to reuse connections\n",
    "    for i in range(1, 5):\n",
    "        url = f'https://so.gushiwen.cn/mingjus/default.aspx?page={i}&tstr=&astr=&cstr=&xstr='\n",
    "        time.sleep(1)\n",
    "        \n",
    "        html = page_request(session, url, ua)\n",
    "        info_list = page_parse(html)\n",
    "        \n",
    "        save_txt('/root/sentence.txt', info_list[1])\n",
    "        \n",
    "        # 处理子网页\n",
    "        print(f\"开始解析第{i}页\")\n",
    "        sub_poem_list = sub_page_request_parse(session, info_list[0], ua)\n",
    "        save_txt('/root/poems.txt', sub_poem_list)\n",
    "    \n",
    "    print(\"******************爬取完成*********************\")\n",
    "    print(f\"共爬取{i}页古诗词名句，保存在如下路径：/root/sentence.txt\")\n",
    "    print(f\"共爬取{i}页古诗词，保存在如下路径：/root/poems.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建和启动mysql指令我写为了一个脚本，这样可以方便的启动和关闭mysql服务，脚本如下：\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# 启动 MySQL 服务\n",
    "service mysql start\n",
    "\n",
    "# 执行 MySQL 命令\n",
    "mysql -u root -p123123 <<EOF\n",
    "CREATE DATABASE IF NOT EXISTS webdb;\n",
    "USE webdb;\n",
    "CREATE TABLE IF NOT EXISTS search_index (\n",
    "    id INT,\n",
    "    keyword CHAR(20),\n",
    "    number INT\n",
    ");\n",
    "ALTER DATABASE webdb CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n",
    "ALTER TABLE search_index CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n",
    "EOF\n",
    "\n",
    "echo \"MySQL database and table setup completed.\"\n",
    "```\n",
    "这个脚本会启动mysql服务，然后创建一个名为webdb的数据库，然后在这个数据库中创建一个名为search_index的表，表中有三个字段，分别是id、keyword和number，然后将数据库和表的字符集设置为utf8mb4。 保存为`setup_mysql.sh`文件。在bash中执行`bash setup_mysql.sh`即可完成数据库的创建和表的创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymysql.cursors\n",
    "\n",
    "# **************************** Begin *********************************\n",
    "# 读取本地HTML文件\n",
    "def get_html():\n",
    "    with open('/data/workspace/myshixun/step3/web_demo.html', 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    return html\n",
    "\n",
    "# 解析HTML文件\n",
    "def parse_html(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    # 找到所有表格中的行\n",
    "    rows = soup.select('table tr')\n",
    "    info_list = []\n",
    "    # 跳过第一行表头\n",
    "    for row in rows[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        rank = int(columns[0].get_text())\n",
    "        keyword = columns[1].get_text()\n",
    "        index = int(columns[2].get_text())\n",
    "        info_list.append((rank, keyword, index))\n",
    "    return info_list\n",
    "\n",
    "# 保存数据库\n",
    "def save_mysql(info_list):\n",
    "    # 连接数据库\n",
    "    connection = pymysql.connect(\n",
    "        host='127.0.0.1',\n",
    "        user='root',\n",
    "        password='123123',\n",
    "        database='webdb',\n",
    "        charset='utf8mb4',\n",
    "        cursorclass=pymysql.cursors.DictCursor\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            # 插入数据\n",
    "            sql = \"INSERT INTO search_index (id, keyword, number) VALUES (%s, %s, %s)\"\n",
    "            cursor.executemany(sql, info_list)\n",
    "        \n",
    "        # 提交事务\n",
    "        connection.commit()\n",
    "        print('成功插入数据')\n",
    "\n",
    "        # 打印插入的数据\n",
    "        # cursor.execute(\"SELECT * FROM search_index\")\n",
    "        # result = cursor.fetchall()\n",
    "        # print('id    keyword    number')\n",
    "        # for row in result:\n",
    "        #     print(f\"{row['id']}    {row['keyword']}    {row['number']}\")\n",
    "    \n",
    "    finally:\n",
    "        # 关闭数据库连接\n",
    "        connection.close()\n",
    "# **************************** End *********************************\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    html = get_html()  # 读取HTML文件\n",
    "    info_list = parse_html(html)  # 解析HTML文件\n",
    "    save_mysql(info_list)  # 保存到MySQL数据库\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
