{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "# *************************************** Begin ******************************************\n",
    "# 请求网页\n",
    "def page_request(url, ua):\n",
    "    try:\n",
    "        response = requests.get(url, headers=ua)\n",
    "        response.raise_for_status()  \n",
    "        response.encoding = response.apparent_encoding\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "# 解析网页\n",
    "def page_parse(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    movie_list = soup.find('ol', class_='grid_view').find_all('li')\n",
    "\n",
    "    for movie in movie_list:\n",
    "        rank = movie.find('em', class_='').text\n",
    "        title = movie.find('span', class_='title').text\n",
    "        rating = movie.find('span', class_='rating_num').text\n",
    "        link = movie.find('a')['href']\n",
    "        print(f'{rank}-{title}-{rating}-{link}')\n",
    "\n",
    "\n",
    "\n",
    "# *************************************** End ******************************************\n",
    "if __name__ == \"__main__\":\n",
    "    print('**************开始爬取豆瓣电影**************')\n",
    "    ua = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4421.5 Safari/537.36'}\n",
    "    # 豆瓣电影Top250每页有25部电影，start就是每页电影的开头\n",
    "    for startNum in range(0, 250, 25):\n",
    "        url = \"https://movie.douban.com/top250?start=%d\" % startNum\n",
    "        html = page_request(url=url, ua=ua)\n",
    "        page_parse(html=html)\n",
    "        print('**************爬取完成**************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import docx\n",
    "from docx.oxml.ns import qn\n",
    "# ********************************************** Begin ************************************************\n",
    "# 请求网页\n",
    "def page_request(url, ua):\n",
    "    try:\n",
    "        response = requests.get(url, headers=ua)\n",
    "        response.raise_for_status()  # 检查请求是否异常\n",
    "        response.encoding = response.apparent_encoding\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "# 解析网页\n",
    "def page_parse(html, ua):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    movie_list = soup.find('ol', class_='grid_view').find_all('li')\n",
    "\n",
    "    for movie in movie_list:\n",
    "        rank = movie.find('em', class_='').text\n",
    "        title = movie.find('span', class_='title').text\n",
    "        link = movie.find('a')['href']\n",
    "        rating = movie.find('span', class_='rating_num').text\n",
    "        rating_count_str = movie.find('div', class_='star').find_all('span')[-1].text\n",
    "        rating_count = int(re.search(r'\\((\\d+)\\)', rating_count_str).group(1))\n",
    "\n",
    "        # 进入子网页，获取每部电影的具体信息\n",
    "        sub_page_requests(link, ua, {\n",
    "            'Rank': rank,\n",
    "            'Title': title,\n",
    "            'Link': link,\n",
    "            'Rating': rating,\n",
    "            'RatingCount': rating_count\n",
    "        })\n",
    "\n",
    "# 子网页处理函数：进入并解析子网页/请求子网页\n",
    "# 获取影片详细信息\n",
    "def sub_page_requests(url, ua, data):\n",
    "    try:\n",
    "        response = requests.get(url, headers=ua)\n",
    "        response.raise_for_status()  # 检查请求是否异常\n",
    "        response.encoding = response.apparent_encoding\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 影片信息\n",
    "        info = soup.find('div', id='info')\n",
    "        director = info.find('span', attrs={'class': 'attrs'}).text.strip()\n",
    "        screenwriter = info.find(text=re.compile('编剧')).next_element.next_element.text.strip()\n",
    "        actors = [actor.text.strip() for actor in info.find_all('span', attrs={'class': 'actor'})]\n",
    "        genre = info.find('span', property='v:genre').text.strip()\n",
    "        release_date = info.find('span', property='v:initialReleaseDate').text.strip()\n",
    "        duration = info.find('span', property='v:runtime').text.strip()\n",
    "\n",
    "        # 影片简介\n",
    "        summary = soup.find('span', property='v:summary').text.strip()\n",
    "\n",
    "        # 更新数据\n",
    "        data.update({\n",
    "            'Director': director,\n",
    "            'Screenwriter': screenwriter,\n",
    "            'Actors': ', '.join(actors),\n",
    "            'Genre': genre,\n",
    "            'ReleaseDate': release_date,\n",
    "            'Duration': duration,\n",
    "            'Summary': summary\n",
    "        })\n",
    "\n",
    "        # 保存影片信息\n",
    "        save(data)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "def save(data):\n",
    "    # 创建Word文档\n",
    "    if not os.path.exists('/root/result'):\n",
    "        os.makedirs('/root/result')\n",
    "\n",
    "    file_path = os.path.join('/root/result', f\"{data['Title']}.docx\")\n",
    "    doc = docx.Document()\n",
    "    \n",
    "    # 设置字体格式\n",
    "    doc.styles['Normal'].font.name = u'Times New Roman'\n",
    "    doc.styles['Normal'].element.rPr.rFonts.set(qn('w:eastAsia'), u'宋体')\n",
    "\n",
    "    # 将爬取到的数据写入word中\n",
    "    doc.add_paragraph(f\"排名: {data['Rank']}\")\n",
    "    doc.add_paragraph(f\"电影名称: {data['Title']}\")\n",
    "    doc.add_paragraph(f\"豆瓣链接: {data['Link']}\")\n",
    "    doc.add_paragraph(f\"评分: {data['Rating']} ({data['RatingCount']}人评价)\")\n",
    "    doc.add_paragraph(f\"导演: {data['Director']}\")\n",
    "    doc.add_paragraph(f\"编剧: {data['Screenwriter']}\")\n",
    "    doc.add_paragraph(f\"主演: {data['Actors']}\")\n",
    "    doc.add_paragraph(f\"类型: {data['Genre']}\")\n",
    "    doc.add_paragraph(f\"上映日期: {data['ReleaseDate']}\")\n",
    "    doc.add_paragraph(f\"片长: {data['Duration']}\")\n",
    "    doc.add_paragraph(f\"剧情简介: {data['Summary']}\")\n",
    "\n",
    "    doc.save(file_path)\n",
    "\n",
    "\n",
    "# ********************************************** End ************************************************\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('**************开始爬取豆瓣电影**************')\n",
    "    ua = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4421.5 Safari/537.36'}\n",
    "    # 豆瓣电影Top250每页有25部电影，start就是每页电影的开头\n",
    "    data_List = []\n",
    "    for startNum in range(0, 251, 25):\n",
    "        url = \"https://movie.douban.com/top250?start=%d\" % startNum\n",
    "        html = page_request(url=url, ua=ua)\n",
    "        # 获取每部影片的信息\n",
    "        page_parse(html=html, ua=ua)\n",
    "    print('**************爬取完成**************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 爬虫相关模块\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 发送邮箱相关模块\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.header import Header\n",
    "\n",
    "# 定时模块\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "# *********************************** Begin ********************************************\n",
    "# 请求网页\n",
    "def page_request(url, header):\n",
    "    try:\n",
    "        response = requests.get(url, headers=header)\n",
    "        response.raise_for_status()  \n",
    "        response.encoding = response.apparent_encoding\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "# 解析网页\n",
    "def page_parse(html):\n",
    "    news = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    hot_searches = soup.find_all('td', class_='td-02')\n",
    "\n",
    "    for search in hot_searches:\n",
    "        name = search.find('a').text\n",
    "        link = search.find('a')['href']\n",
    "        heat = search.find('span', class_='td-02').text\n",
    "        news.append({'name': name, 'link': link, 'heat': heat})\n",
    "    \n",
    "    return news\n",
    "\n",
    "def sendMail(news):\n",
    "    sender = 'your_email@example.com'  \n",
    "    receiver = 'receiver_email@example.com'  \n",
    "    smtp_server = 'smtp.example.com'  \n",
    "    smtp_port = 25  \n",
    "    password = 'your_password'  \n",
    "\n",
    "    message = '热搜信息如下：\\n\\n'\n",
    "    for n in news:\n",
    "        message += f'热搜名称: {n[\"name\"]}\\n链接: {n[\"link\"]}\\n实时热度: {n[\"heat\"]}\\n\\n'\n",
    "\n",
    "\n",
    "    msg = MIMEText(message, 'plain', 'utf-8')\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    msg['Subject'] = Header('微博热搜榜', 'utf-8')\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, smtp_port)\n",
    "        server.login(sender, password)\n",
    "        server.sendmail(sender, [receiver], msg.as_string())\n",
    "        server.quit()\n",
    "        print(\"邮件发送成功！\")\n",
    "    except Exception as e:\n",
    "        print(f\"邮件发送失败：{e}\")\n",
    "# *********************************** End ********************************************\n",
    "\n",
    "def job():\n",
    "    print('**************开始爬取微博热搜**************')\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'zh-CN,zh-Hans;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Cookie': 根据各自的浏览器中显示的填写\n",
    "    }\n",
    "\n",
    "    url = 'https://s.weibo.com/top/summary'\n",
    "    html = page_request(url=url, header=header)\n",
    "    page_parse(html)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# 定时爬取，每隔20s爬取一次微博热搜榜并将爬取结果发送至个人邮箱\n",
    "# 可以将20修改成其他时间\n",
    "    schedule.every(20).seconds.do(job)\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
