数据清洗一般是应用在几个数据库合并时或者多个数据源进行集成时。
数据仓库是为了支持决策分析的数据集合。
数据清洗的主要应用领域包括数据仓库与数据挖掘、数据质量管理。
数据清洗按照实现方式，可以分为手工清洗和自动清洗。
数据清洗中缺失值处理常用的处理方法有估算、整例删除、变量删除、成对删除四种。
数据清洗主要是对缺失值、重复值、异常值和数据类型有误的数据进行处理。
数据清洗的基本流程一共分为5个步骤，分别是数据分析、定义数据清洗的策略和规则、搜寻并确定错误实例、纠正发现的错误、干净数据回流。
数据清洗的评价标准包括数据可信性、数据可用性、数据清洗的代价。
常见的数据转换策略包括平滑处理、聚集处理、数据泛化处理、规范化处理、属性构造处理。
平滑处理旨在帮助去掉数据中的噪声，常用的方法包括分箱、回归、聚类等。
规范化处理是一种重要的数据转换策略，它是将一个属性取值范围投射到一个特定范围之内，以消除数值型属性因大小不一而造成挖掘结果的偏差。
对箱子的划分方法一般有两种，一种是等高方法，另一种是等宽方法。
通过聚类分析方法可帮助发现异常数据，那些位于这些聚类集合之外的数据对象，则被认为是异常数据。
常用的规范化处理方法包括Min-Max规范化、Z-Score 规范化、小数定标规范化等。
数据转换策略中，Min-Max规范化方法是对被转换数据进行一种线性转换。
Min-Max规范化方法的转换公式是x=(待转换属性值−属性最小值)/(属性最大值−属性最小值)。
假设属性的最大值和最小值分别是87000元和11000元，现在需要利用Min-Max规范化方法，将“顾客收入”属性的值映射到0~1 的范围内，则“顾客收入”属性的值为72400元时，对应的转换结果是(72400-11000)/(87000-11000)=0.808。
数据转换策略中，小数定标规范化方法是通过移动属性值的小数位置来达到规范化的目的。
Z-Score规范化的转换公式是z=(待转换属性值−属性平均值)/属性标准差。
在爬取网页的时候，输出的信息中有时候会出现“抱歉，无法访问”等字眼，这就是禁止爬取，需要通过定制请求头(Headers)来解决。
网络请求不可避免会遇上请求超时的情况，可以为requests的timeout参数设定等待秒数。
BeautifulSoup是一个HTML/XML的解析器，主要功能是解析和提取HTML/XML数据。
BeautifulSoup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为Tag对象、NavigableString对象、BeautifulSoup对象、Comment对象四种。
BeautifulSoup中，Tag对象就是HTML中的一个个标签。
Tag对象有两个重要的属性，即name属性和attrs属性。
BeautifulSoup中，NavigableString对象用于操纵字符串。
BeautifulSoup中，BeautifulSoup对象表示的是一个文档的全部内容，大部分时候，可以把它当作一个特殊的Tag对象。
BeautifulSoup中，Comment对象是一种特殊类型的NavigableString对象，输出的内容不包括注释符号。
从HTML中找到想要的数据，BeautifulSoup提供了两种方式，一种是遍历文档树，另一种是搜索文档树。
BeautifulSoup中，遍历文档树就是从根节点html标签开始遍历，直到找到目标元素为止。
BeautifulSoup中，Tag对象的contents属性可以将某个Tag对象的子节点以列表的方式输出。
BeautifulSoup中，Tag对象的children属性是一个迭代器，可以使用for循环进行遍历。
BeautifulSoup中，Tag对象的stripped_strings属性，可以获得去空白行的标签内的众多内容。
BeautifulSoup中，使用Tag对象的parent属性可以获得父节点，使用Tag对象的.parents属性可以获得从父节点到根节点的所有节点。
BeautifulSoup中，可以使用Tag对象的next_sibling属性和.previous_sibling属性分别获取下一个兄弟节点和上个兄弟节点。
BeautifulSoup中，可以使用Tag对象的next_siblings属性和.previous_siblings属性对当前的兄弟节点迭代输出。
BeautifulSoup中，可以使用Tag对象的next_element属性和.previous_element属性用于获得不分层次的前后元素。
BeautifulSoup中，可以使用Tag对象的next_elements属性和.previous_elements属性可以向前或向后解析文档内容。
BeautifulSoup中，find()和find_all()方法可以通过指定标签的属性值来精确定位某个节点元素。
BeautifulSoup中，Tag对象或BeautifulSoup对象的select()方法，可以使用CSS选择器的语法找到标签。
BeautifulSoup自动将输入文档转换为Unicode编码，将输出文档转换为UTF-8编码。
BeautifulSoup中，Python标准库的HTML解析器用法是BeautifulSoup(markup,"html.parser")。
BeautifulSoup中，lxml库的HTML解析器用法是BeautifulSoup(markup,"lxml")。
BeautifulSoup中，lxml库的XML解析器用法是BeautifulSoup(markup,"lxml-xml")或BeautifulSoup(markup,"xml")。
BeautifulSoup中，在使用xml库的解析器之前，需要安装lxml库，其命令是pip install lxml。
XPath中，选取所有body下第k个p标签的语法是//body/p[k]。
XPath中，选取所有body下最后一个p标签的语法是//body/p[last()]。
XPath中，选取所有body下倒数第二个p标签的语法是//body/p[last()-1]。
XPath中，选取所有body下的前两个p标签的语法是//body/p[position()<3]。
XPath中，选取所有body下带有class属性的p标签的语法是//body/p[@class]。
XPath中，选取所有body下class为bigdata的p标签的语法是//body/p[@class="bigdata"]
XPath中，//p[contains(@class, "bigdata")]函数可以选取所有class属性包含bigdata的p标签。
XPath中，//a[starts-with(@class, "bigdata")]函数可以选取所有class属性以bigdata开头的a标签。
Kafka是一个分布式流处理平台，具有高吞吐量、高可靠性和低延迟等特性。
Kafka可以对生产者和消费者实现解耦，并可以缓存消息。
对于消息系统而言，一般有点对点消息传递模式和发布订阅消息传递模式两种主要的消息传递模式。
在发布订阅消息传递模式中，消息的生产者称为“发布者”(Publisher)，消费者称为“订阅者”(Subscriber)。
Kafka中的消息是以日志的形式进行存储的。
Kafka集群包含一个或多个服务器，这些服务器被称为“Broker”。
每条发布到Kafka集群的消息都有一个类别，这个类别被称为“Topic (主题)”。
消息生产者(Producer)负责发布消息到Kafka的Broker。
消息消费者(Consumer)负责从Kafka的Broker读取消息的客户端。
Kafka中每个Consumer属于一个特定的Consumer Group。
Kafka中可为每个Consumer指定Group Name，若不指定，则该Consumer属于默认的Group。
Kafka中同一个Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费。
Kafka通过Zookeeper管理集群配置。
Kafka中Producer使用推(push)模式将消息发布到Broker，Consumer使用拉(pull)模式从Broker订阅并消费消息。
在使用Python操作Kafka之前，需要安装第三方模块python-kafka，命令是pip install kafka-python。
Flume最小独立运行单位是Agent。
Agent核心包含三个组件，分别是数据源(Source)、数据通道(Channel)、数据槽(Sink)。
使用Flume的核心是设置配置文件，在配置文件中定义Source、Sink和Channel的相关信息。
在Flume中，数据传输的基本单位是event。
一个典型的数据仓库系统通常包含数据源、数据存储和管理、OLAP服务器、前端工具和应用四个部分。
实时主动数据仓库可实时捕捉数据源中发生的变化。
实时主动数据仓库中，数据集成方式包括数据整合、数据联邦、数据传播、混合方式四种。
数据分发是数据集成过程的一个重要组成部分。目前，大致存在推(push)和拉(pull)、周期和非周期、一对一和一对多几种数据分发方式。
ETL是指将业务系统的数据抽取(Extract)、转换(Transform)、加载(Load)到数据仓库的过程。
数据的抽取可以采用周期性的“拉”机制或者事件驱动的“推”机制，两种机制都可以充分利用CDC技术。
数据转换可能包括数据重构和整合、数据内容清洗或集成。
变化数据捕捉(Change Data Capture，CDC)可以实现实时高效的数据集成，是实时主动数据仓库连续数据集成的有效解决方案。
CDC的特性主要包括没有宕机时间、保持数据新颖性、减少系统开销。
CDC包括变化捕捉代理、变化数据服务和变化分发机制三个组成部分。
CDC有两个典型的应用场景分别是面向批处理的CDC(pull CDC)和面向实时的CDC(push CDC)。
Kettle中，Spoon是一个图形用户界面，可以方便、直观地完成数据转换任务。
Kettle中，Spoon可以运行转换(.kst)或者任务(.kjb)，其中，转换用Pan来运行，任务用Kitchen来运行。
一个Kettle数据抽取过程主要由作业(Job)构成。
每个Kettle作业由一个或多个作业项(JobEntry)和连接作业项的作业跳(Job Hop)组成。
作业跳是作业项之间带箭头的连接线，它定义了作业的执行路径。
Kettle中转换主要用于数据的抽取(Extraction)转换(Transformation)及加载(Load)。
Kettle中步骤之间的数据以数据流方式传递。
在pandas中mean()函数可以计算值的平均数。
在pandas中median()函数可以计算值的算术中位数(50%分位)。
在pandas中mad()函数可以根据平均值计算平均绝对离差。
在pandas中var()函数可以计算样本值的方差。
在pandas中std()函数可以计算样本值的标准差。
在pandas中cumsum()函数可以计算样本值的累计和。
pandas提供了isnull()和notnull()函数，可以更容易地检测缺失值。
pandas提供了各种方法来清除缺失的值，fillna()函数可以通过指定值和插值的方法填充缺失数据。